train_file: ['handa/data_filter_tra_train_new.json']
#test_file: ['handa/data_filter_tra_test.json']
#test_file: ['handa/cases_com_tra_mid.json']
test_file: ['handa/log_case_test_52_data.json']
#test_file: ['handa/data_filter_tra_all_com_mid.json']
#test_file: ['handa/cases_com_tra_mid_combine.json']  # 1648 complete combine data
#test_file: ['handa/cases_com_tra_mid_new.json']  # 1067 complete data
#test_file: ['handa/cases_com_tra_mid_new_not_perfect.json']  # 156 not perfect complete data
# each train_file (json) contains a python list where each item is {'image': img_path, 'caption': text or list_of_text }
bert_config: './configs/config_wwm.json'
data_prefix: '/data/private/songchenyang/hanzi_filter'
model: 'SingleMlm'
output_path: 'output/tra_finetune_single_mlm_p0_load_image_mk25_unrec_wwm_all_noinit_mlm_rec10_b4'
# vocab_file: ../chinese-bert-wwm-ext/vocab.txt
# vocab_size: 4116

image_res: 128
batch_size: 4
img_mode: 'RGB'
dataset_mode: 'normal'
modality: 'image'
predict_all: true
target_weight: 10.0  # ALERT: special dividing weight
extra_mlp: false
mlp_dim: [768, 512, 768]
tradition_mlm: true
tradition_mlm_weight: 1.0
img_random_transform: true
img_mask_ratio: 0.25
img_mask_policy: 'single'
img_noise_ratio: -1
img_do_rotate: false
pad_color: 0
topk: [1, 5, 10, 20]
specific_test: true
max_length: 16

visual_encoder: 'vit'
encoder_layer: 12
image_reconstruct_factor: 10.0
decoder_layer: 8
mlm_probability: -1

optimizer: {opt: adamW, lr: 1e-5, weight_decay: 0.02}
schedular: {sched: cosine, lr: 1e-5, epochs: 60, min_lr: 1e-5, decay_rate: 1, warmup_lr: 1e-5, warmup_epochs: 20, cooldown_epochs: 0}
